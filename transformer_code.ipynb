{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this ipynb owes hugely to [Luis Fernando Torres](https://www.kaggle.com/lusfernandotorres/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code and my comment, i hope you will not need to seek for other aiding information to understand fully a transformer.\n",
    "\n",
    "Nah, i was kidding, it was impossible.\n",
    "\n",
    "However, you should read the code and comments in this repo from top to bottom. Don't do this in a utilitarian way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the `scratch`\n",
    "import the prerequisite modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch and python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import math\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Union, Tuple\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major function of torch is to deploy python's class to manipulate networks' layers without knowing details of GPU operations.\n",
    "\n",
    "`torch.nn` makes neural network layer easily edited.\n",
    "\n",
    "`torch.utils.data` makes data i/o simple, where :\n",
    "- `Dataset` is usually a paternal class to be inherited by customized dataset class. It is differnt from `datasets` that is a hub of many dataset data.\n",
    "- `DataLoader` is aimed at giving out batches of data of certain format. It is dealing with trainer directly.\n",
    "- `random_split` is a tool to split train/test randomly to a database.\n",
    "    \n",
    "`torch.utils.tensorboad` is a typical command that registers what is happening during training. It has almost never reveals its necessity during my personal experience of training. I would rather look at the loss as part of training history either in loss plot or log.\n",
    "\n",
    "On pythonic level:\n",
    "- `math` is useful in square root and logarithm.\n",
    "- `tqdm` is a time visual indicator so that during iterative training, you have something to see other than a blank sheet of piece of screen.\n",
    "- `typing` is a trend of pythonic writing for a better constraint of type and format.\n",
    "- `Path` from `pathlib` is an option for many coders, though I prefer the one in `os`. \n",
    "\n",
    "Matter of fact, the thing that is done on the level of python is better if consulting Gemini and stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace libraries \n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On HuggingFace level:\n",
    "- `datasets` usually refers to the clean data downloadable from official mirrors. People usually use them in tutorials and demo case. There is usually available split of train, test, validation.\n",
    "- here `toeknizers` provides many deatiled stuff to be imported. In many lazy codes, the tokenizer is usually called via:\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_basic_tokenization = True)\n",
    "```\n",
    "or other llm foundation models to substitute in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From 'scratches' to blocks!\n",
    "Suppose you are familiar with transformer architecture now. If you're not, go google it by yourself. In this section, we will see 5 fundamental blocks which is the foundation of bigger blocks: \n",
    "- Input Embedding\n",
    "- Positional Embedding\n",
    "- Add & Norm\n",
    "- Feed Forward\n",
    "- Multi-Head Attention\n",
    "\n",
    "It is noted that these above 5 blocks by themselves are established upon toolkits from modules like `nn.Linear`, `nn.Dropout`, etc. as well as basic pytorch and python operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Embedding: from tokens to embedings\n",
    "class InputEmbedings(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self, d_model: int, vocab_size: int): # two parameters only, d_model and vocab_size\n",
    "        super().__init__() # should let your IDE know before you actually type them all.\n",
    "        self.d_model = d_model # as detourable initialization, you have to do the self.xxx = xxx to all inputting parameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # usually people don't care too much on the realizatio fashion in embedding. They prefer deploy nn.Embedding directly here.\n",
    "\n",
    "    def forward(self, x): # in a nn.Module, there is always __init__ , forward.\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # this is for normalzing the variacne of the embedding, incase they are extremely small. Can't argue with  popular protocols.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coding for positional embedding is very fascinating if it were your first time to see this.\n",
    "However, the enthusiasm drops after you seen too many of them.\n",
    "Some variants of positional embedding is interesting indeed, as in some fancy papers.\n",
    "For the record, the mathematic formula for the vanilla positional encoding is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\text{Even Indices } (2i): \\quad \\text{PE(pos, } 2i) = \\sin\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n",
    "    \\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\text{Odd Indices } (2i + 1): \\quad \\text{PE(pos, } 2i + 1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The positional embedding is used to add upon original embeddings.\n",
    "Intuitively, for a certain dimension among `d_model`, it became similar to a neighbor that is a certain range of integers far. This certain rainges determines the 'capability' of detecting *something worth detecting*, which is equivalent to the idea of *attention* if you think about it. Since've explained what is the denominator doing inside the trigonometric brackets, now what about the `sin` and `cos`? Again, intuitively, this is for symmetric thought, an ideology deeply rooted in fourier transform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Positional Encoding\n",
    "class PositionalEncoding(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None: # three parameters only here. In some cases, seq_len is named as max_len.\n",
    "        super().__init__() # now your IDE knows before you actually type them all.\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout) # the Dropout layer is usually used in the transformer, people inherit from nn directly.\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model) # pe as in positional encoding; it should be the same shape with *each* embedded tokens.\n",
    "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # the multiplied variable. a 2D tensor['seq_len, 1'].  Shape: (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # shape:(d_model, )\n",
    "        # position[i] = i\n",
    "        # div_term[j] = exp(j * (-log(10000.0)) / d_model), ...\n",
    "        #             = exp(-log(10000.0) * j / d_model)\n",
    "        #             = 10000.0^(-j/d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # note the j is even number. So, if j = 2k, where k is continuous number between 0 and half max_len, then position * div_term should be: i * (10000.0^(-2/d_model))^k, which is easier to programme.\n",
    "\n",
    "        pe = pe.unsqueeze(0) # an added dimension for 'batch' dimension when adding-up.\n",
    "        self.register_buffer('pe', pe) # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter, i.e. untrainable.\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)  # The  `requires_grad_(False)`` ensures that the positional encoding is not updated during backpropagation, treating it as a constant.\n",
    "        return self.dropout(x) # a quite common practice to apply dropout to output in a concise way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dropout vs residual\n",
    "A dropout layer is a regularization technique commonly used in neural networks. It randomly \"drops out\" a certain percentage of neurons during training, preventing the network from becoming overly reliant on any particular neuron. This helps to reduce overfitting and improve generalization.\n",
    "\n",
    "During training, each neuron has a probability (usually set between 0.2 and 0.5) of being \"dropped out.\"\n",
    "If a neuron is dropped out, its output is set to zero.\n",
    "During testing, all neurons are active, but their outputs are scaled down by the dropout probability to compensate for the training-time dropout.\n",
    "\n",
    "A residual layer, also known as a skip connection, is a technique used to improve the training of deep neural networks. It allows information to flow directly from earlier layers to later layers, helping to prevent the vanishing gradient problem and making it easier for the network to learn deep features.\n",
    "\n",
    "A residual layer consists of two parts: a stack of layers (e.g., convolutional layers, fully connected layers) and a shortcut connection.\n",
    "The shortcut connection directly adds the input of the stack to the output of the stack.\n",
    "This allows the network to learn the residual function, which represents the difference between the desired output and the input.\n",
    "\n",
    "\n",
    "Key Differences\n",
    "\n",
    "- Purpose: Dropout is primarily used for regularization to prevent overfitting, while residual layers are used to improve the training of deep networks.\n",
    "- Mechanism: Dropout randomly drops out neurons, while residual layers add a shortcut connection.\n",
    "- Impact: Dropout affects the training process by introducing noise, while residual layers affect the network architecture.\n",
    "In summary:\n",
    "\n",
    "Dropout is a regularization technique that helps prevent overfitting by randomly dropping out neurons.\n",
    "Residual layers are a technique used to improve the training of deep networks by adding shortcut connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch multiplication, trailing singleton, and broadcasting.\n",
    "PyTorch follows a set of rules to determine when and how to expand tensors during broadcasting operations. These rules are:\n",
    "\n",
    "Trailing Singleton Dimensions: If one tensor has a trailing singleton dimension (a dimension with size 1), it can be expanded to match the corresponding dimension of the other tensor.\n",
    "Compatible Dimensions: The dimensions of the tensors must be compatible. This means that they must either be equal or one of them must be 1.\n",
    "In the case of (3,1) * (4,):\n",
    "- The tensor of shape (4,) has a trailing singleton dimension: (4,) is equivalent to (1, 4).\n",
    "- The dimensions (3, 1) and (1, 4) are now compatible.\n",
    "\n",
    "Another example for broadcasting:\n",
    "Scenario:\n",
    "- Tensor A: Shape (2, 3)\n",
    "- Tensor B: Shape (3,)\n",
    "\n",
    "Goal: Multiply Tensor A and Tensor B.\n",
    "\n",
    "Explanation:\n",
    "- Dimension Compatibility: The dimensions of Tensor A and Tensor B are compatible because the last dimension of Tensor A (3) matches the only dimension of Tensor B (3).\n",
    "- Broadcasting: PyTorch will automatically expand Tensor B to (1, 3) to match the first dimension of Tensor A.\n",
    "- Element-wise Multiplication: Now, both tensors have the same shape (2, 3), and PyTorch can perform element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add & Norm (i.e. LayerNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add & Norm\n",
    "class LayerNormalization(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self,  eps: float = 10**-6) -> None: \n",
    "        super().__init__() # your IDE knows it now, i am sure about it.\n",
    "        self.eps = eps\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # the scale parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # the shift parameter\n",
    "        # the two parameters are to be trianed.\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # in a nn.Module, there is always __init__ , forward.\n",
    "        mean = x.mean(-1, keepdim = True) # the mean along the last dimension, the dimension of d_model\n",
    "        std = x.std(-1, keepdim = True) # the standard deviation along the last dimension, the dimension of d_model\n",
    "        # 'keepdim = True' ensures that the output has the same shape as the input. here the dimension to be mean'ed/std'ed is 1.\n",
    "    \n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer, Group, and Batch Normalization: A Comparison\n",
    "\n",
    "Normalization techniques are essential for training deep neural networks, as they help to stabilize the learning process and improve generalization. Layer, Group, and Batch normalization are common methods used for this purpose.\n",
    "\n",
    "Layer Normalization (LN)\n",
    "- Normalization: Normalizes the input to a layer across all dimensions.\n",
    "- Benefits: Can be more stable than batch normalization for recurrent neural networks or when batch sizes are small.\n",
    "- Drawbacks: Can be less effective for convolutional neural networks.\n",
    "\n",
    "Group Normalization (GN)\n",
    "\n",
    "Normalization: Normalizes the input to a layer across a group of channels.\n",
    "- Benefits: Can be more stable than batch normalization when the batch size is small or when dealing with data with varying numbers of channels.\n",
    "- Drawbacks: May not be as effective as batch normalization for some tasks.\n",
    "- Batch Normalization (BN)\n",
    "\n",
    "Normalization: Normalizes the input to a layer across the batch dimension.\n",
    "- Benefits: Helps to stabilize the learning process and can improve the convergence speed of training.\n",
    "- Drawbacks: Can be sensitive to small batch sizes and may not be as effective for recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward\n",
    "class FeedForwardBlock(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None: \n",
    "        super().__init__() # your IDE knows it now.\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)  #  The self.linear1 layer has two parameters: weight and bias. \n",
    "        self.dropout = nn.Dropout(dropout)  # The self.dropout layer does not have any parameters that need to be trained.\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        # 4 parameters to be trained.\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # in a nn.Module, always __init__ , forward.\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        # the above comment format is prevalent and magnificent.\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x)))) # this is a concise way of applying linears, dropouts, and relus.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention\n",
    "class MultiHeadAttentionBlock(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h # head counts\n",
    "\n",
    "        assert d_model % h == 0 # assert the d_model is divisible by h\n",
    "        self.d_k = d_model // h # d_k is the dimension of each attention head's key, query, and value vectors\n",
    "        # every head occupies some depths of the total d_model.\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model) # W_q, same shape with q\n",
    "        self.w_k = nn.Linear(d_model, d_model) # W_k, same shape with k\n",
    "        self.w_v = nn.Linear(d_model, d_model) # W_v, same shape with v\n",
    "        self.w_o = nn.Linear(d_model, d_model) # W_o, same shape with o. This matrix is used in final result.\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # dropout is almost everywhere.\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout): # people use this for obtaining atten score and stuffs.\n",
    "        d_k = query.shape[-1] # people don't use the one in class initialization\n",
    "        attention_scores =  (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # note we are doing things under 1 head\n",
    "        # the batch dimension is always there\n",
    "        # @ is for matrix multiplication\n",
    "        # this is a strict realization of the mathematical formula for attention score BEFORE softmax\n",
    "\n",
    "        if mask is not None: # the mask issue is in both self- and cross- attention mechansim.\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9) # masking, or inf in other tutorials I read...for example, the elements above the diagonal would be corrupted intentionally.\n",
    "            # masked_fill is actually a method of the Tensor class.\n",
    "\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax, with dim being -1.\n",
    "        # softmax(dim=-1) can be applied to a tensor of any shape.\n",
    "\n",
    "        attention_scores = dropout(attention_scores) # dropout is everywhere.\n",
    "        return (attention_scores @ value), attention_scores \n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        # split the last dimension of query into h heads, and then bring head to 2nd dimension\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0], x.shape[1], self.d_model) # recover!!\n",
    "\n",
    "        return self.w_o(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From blocks to bigger blocks!\n",
    "We build three bigger blocks from here:\n",
    "- Residual Connection\n",
    "- EncoderBlock/Encoder with many EncoderBlocks\n",
    "- DecoderBlock/Decoderwith many DeocoderBlocks\n",
    "\n",
    "## Residual Connection\n",
    "This concept has been compared with dropout in the above context.\n",
    "\n",
    "Residual Connection bases itself upon LayerNormalizaiton and corresponding `sublyaer` with `nn.Dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual connection\n",
    "class ResidualConnection(nn.Module): \n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, sublayer): # its key difference against dropout lies in its involvement of   `sublayer`.\n",
    "        return x + self.dropout(sublayer(self.norm(x))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderBlock and Encoder\n",
    "An EncoderBlock repeats itself in an Encoder.\n",
    "\n",
    "EncoderBlock bases itself upon MultiHeadAttention and FeedForward as well as ResidualConnection. Encoder is built with EncoderBlocks with LayerNormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EncoderBlock\n",
    "class EncoderBlock(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "        # nn.ModuleList is a concise way of expressing multiple layers of neural networks.\n",
    "        # The reason ResidualConnection is not an input parameter is that it's not a variable component that needs to be customized or injected from outside. \n",
    "        # The ResidualConnection is a fixed part of the encoder block architecture, and its configuration (i.e., the dropout rate) is already determined by the dropout parameter.\n",
    "        # The MultiHeadAttention and FeedForwardBlock, however, they are dependent on an inputting x, which means they are not fixed beforehand.\n",
    "        \n",
    "    def forward(self, x, src_mask): # x is usually an input tensor\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) \n",
    "        #  self.self_attention_block(……: in pytorch, the for an instance of a class xxx, the `xxx()` is actually calling `xxx.forward()`.\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block) # since FeedForward has only 1 param in its forward method.\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self, layers: nn.ModuleList) -> None: # again, nn.ModuleList, indicating there are multiple layers of things.\n",
    "        # however, the conent of the layers is not specified here, though it is EncoderBlock, matter of fact.\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecoderBlock and Decoder\n",
    "similar to its En- counterparts, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecoderBlock\n",
    "class DecoderBlock(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block # routines of copy\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x, tgt_mask)) # self attention with target mask\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask)) # cross attention with source mask\n",
    "        # in cross-attention, query comes from target; key and finally value come from source, i.e. encoder's outputs. Goal: for a given query, find the tokens' relevance towards it.\n",
    "        # The source mask is used to prevent the model from attending to irrelevant or future information in the source sequence. \n",
    "        # It is typically applied to the attention scores before the weighted sum is computed.\n",
    "        # mask things are realized in loading dataset, which ahas causal_mask and padding_mask types.\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder \n",
    "class Decoder(nn.Module): # as usuall, you need to inherit nn.Module\n",
    "    def __init__(self, layers: nn.ModuleList) -> None: # again, nn.ModuleList, indicating there are multiple layers of things.\n",
    "        # however, the conent of the layers is not specified here, though it is DecoderBlock, matter of fact.\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and softmax, in fine, i.e. ProjectionLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer and softmax\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        # Je vous rapelle que le 'd_model' est un bas-dimeinsonelle truc qui represente le meme chose que vocab_size, essentiellement.\n",
    "        # so it is now time to regerate backa to realworld vocab, projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.linear(x), dim = -1)\n",
    "        # recall that\n",
    "        # softmax(x_i) = exp(x_i) / sum(exp(x))\n",
    "        # log_softmax(x_i) = log(exp(x_i) / sum(exp(x)))\n",
    "        # Numerical Stability: Log-softmax can be more numerically stable than softmax, especially when dealing with very large or very small values.\n",
    "        # Loss Functions: Log-softmax is often used with cross-entropy loss, while softmax can be used with other loss functions like mean squared error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build the Transformer now!!!\n",
    "First, we need to accomplish the class of Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, encoder: Encoder, decoder: Decoder, \n",
    "            src_embed: InputEmbedings, tgt_embed: InputEmbedings,\n",
    "            src_pos: PositionalEncoding, tgt_pos: PositionalEncoding,\n",
    "            projection_layer: ProjectionLayer\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer # till now every part in the picture of transformer is covered!\n",
    "\n",
    "    def encode(self, src, src_mask): # combines source embedding, source postional embedding, and source encoder\n",
    "        return self.encoder(self.src_pos(self.src_embed(src)), src_mask)\n",
    "        \n",
    "    def decode(self, encoder_ouptut, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_pos(self.tgt_embed(tgt)), encoder_ouptut, src_mask, tgt_mask)\n",
    "\n",
    "    def project(seff, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we initialize each class mentioned in the whole project, which is to initialize them according to the inputting parametric list in `__init__`.\n",
    "\n",
    "You can see that the `forward` method inherits the `forward` method of which the class it belongs to inherits.\n",
    "\n",
    "It is noted that it is not that when the class of `transformer` is constructed then everything is initialized: no, everything is just a skeleton without being initialized now. \n",
    "\n",
    "The initialization requires the first driving force from hyperparamaters, the parameters that is required by the `__init__`s in each class.\n",
    "\n",
    "To look in the other way around, only when every involved class is initialized from the point of view of `transformer` can a `transformer` be able to be initialized.\n",
    "\n",
    "We do this sort of initialization via function `build_transformer`. \n",
    "It is noted that \n",
    "class of `ResidualConnection` needs not  initalization because it is defined immediately they are applied (e.g. in EncoderBlock);\n",
    "class of `LayerNormalization`, either, because it is used with default inputting parameters immediately where it is applied.\n",
    "\n",
    "In the objective of `build_transformer`, its inputting parameters should be the collection of the *init-requiring* parameters. To make clear on that, let's make a table for that:\n",
    "\n",
    "\n",
    "| class | init-requiring | \n",
    "|---|---|\n",
    "| InputEmbeding | d_model: int, vocab_size: int | \n",
    "| PositionalEncoding | d_model: int, seq_len: int, dropout: float| \n",
    "| LayerNormalization | eps: float = 10**-6| \n",
    "| FeedForwardBlock   | d_model: int, d_ff: int, dropout: float |\n",
    "| MultiHeadAttention | d_model: int, h: int, dropout: float |\n",
    "| ResidualConnection | dropout: float |\n",
    "| EncoderBlock | self_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float |\n",
    "| Encoder | layers: nn.ModuleList |\n",
    "| DecoderBlock | self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float |\n",
    "| Decoder | layers: nn.ModuleList |\n",
    "| ProjectionLayer | d_model: int, vocab_size: int |\n",
    "| Transformer | encoder: Encoder, decoder: Decoder, src_embed: InputEmbedings, tgt_embed: InputEmbedings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding,projection_layer: ProjectionLayer |\n",
    "\n",
    "\n",
    "\n",
    "N: int = 6, \n",
    "\n",
    "Therefore, we come to a minimum list of inputting parameter for `build_transformer` (with assigned default values):\n",
    "\n",
    "| inputting parameters |  \n",
    "|---|\n",
    "| d_model: int = 512, (conventional choice) |\n",
    "| src_vocab_size: int,tgt_vocab_size: int (imagine two language situations) |\n",
    "| src_seq_len: int, tgt_seq_len: int  (ditto) |\n",
    "| dropout: float = 0.1 (conventional choice) |\n",
    "| h: int = 8 (512/8=64, sounds harmonious) |\n",
    "| d_ff: int = 2048 (ditto) |\n",
    "\n",
    "There is another parameter to be assigned, the layers of EncoderBlocks inside an Encoder:N: int = 6.\n",
    "Matter of fact, the relation between De-/En-coderBlocks and  De-/En-coders is quite loose.\n",
    "Need to fulfill the `layers: nn.ModuleList`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_transformer\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    src_embed = InputEmbedings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbedings(d_model, tgt_vocab_size)\n",
    "\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len)\n",
    "    \n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # the mask things occurs only in forward method, therefore no showing up in initialization.\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    \n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(encoder_block)\n",
    "\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # now we initialize everything!!! use a conventional initializaiton protocol:\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1: # only parameters with more than one dimension are initialized using the Xavier uniform initialization. This is because Xavier initialization is specifically designed for matrices and tensors with multiple dimensions, and applying it to scalars (1-dimensional tensors) would be unnecessary and potentially harmful.\n",
    "            nn.init.xavier_uniform_(p)\n",
    "            #  The purpose of Xavier uniform initialization is to ensure that the variance of the weights remains roughly the same across different layers, which helps to stabilize the training process.\n",
    "            # f you don't explicitly initialize parameters in PyTorch, they will be initialized with random values. By default, PyTorch uses a uniform distribution to initialize weights and a zero initialization for biases.\n",
    "    return transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section and the following is not the focus of the blog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "def get_all_sentences(ds, lang):\n",
    "    for pair in ds:\n",
    "        yield pair['translation'][lang]\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "def build_tokenizer(config, ds, lang):\n",
    "    # Create a file path for the tokenizer \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    \n",
    "    if not Path.exists(tokenizer_path): \n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n",
    "        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
    "        \n",
    "        # Creating a trainer for the new tokenizer\n",
    "        trainer = WordLevelTrainer(\n",
    "            special_tokens = [\"[UNK]\", \"[PAD]\",  \"[SOS]\", \"[EOS]\"], \n",
    "            min_frequency = 2  # Only include tokens that appear at least 2 times in the training data\n",
    "        ) # Defining Word Level strategy and special tokens\n",
    "        \n",
    "        # Training new tokenizer on sentences from the dataset and language specified \n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
    "        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
    "    return tokenizer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in many simple cases, tokenizer is directly used as:\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_basic_tokenization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def get_ds(config):\n",
    "    # Loading the train portion of the OpusBooks dataset.\n",
    "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
    "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train') \n",
    "    \n",
    "    # Building or loading tokenizer for both the source and target languages \n",
    "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "    \n",
    "    # Splitting the dataset for training and validation \n",
    "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
    "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
    "                                    \n",
    "    # Processing data with the BilingualDataset class, which we will define below\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "                                    \n",
    "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for pair in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "        \n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "    # Creating dataloaders for the training and validadion sets\n",
    "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`datasets` module provides a lot of datasets that can be checked via:\n",
    "\n",
    "```python\n",
    "from datasets import list_datasets\n",
    "available_datasets = list_datasets()\n",
    "print(available_datasets)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
